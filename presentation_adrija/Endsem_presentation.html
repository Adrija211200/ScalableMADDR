<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>A Scalable Version of MADD</title>
    <meta charset="utf-8" />
    <meta name="author" content="Adrija Saha, Roll No: MD2203" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="design.css" type="text/css" />
    <link rel="stylesheet" href="fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# A Scalable Version of MADD
]
.subtitle[
## for Classification Problems
]
.author[
### Adrija Saha, Roll No: MD2203
]
.institute[
### Supervisor: Dr. Soham Sarkar (SMU, ISI Delhi)
]
.date[
### 23/05/2024
]

---


class: center, middle
background-image: url("BACK.jpg")
background-size: cover

&lt;style type="text/css"&gt;
.reduced_opacity {
  opacity: 0.4;
}
.red { color: rgb(200,0,0); }
.green { color: green; }
.blue { color: blue; }
.scroll-1000 {
  max-height: 400px;
  max-width: 1000px;
  overflow-y: auto;
  background-color: inherit;
}
&lt;/style&gt;



&lt;style type="text/css"&gt;
.remark-slide-number {
  display: none;
}
&lt;/style&gt;

# Last time...

---
# Introduction

- We were interested in **Classification** problems.

- A popular choice is to consider **K-Nearest Neighbor Classifier** based on the Euclidean distance.

- But in High Dimensional problems, KNN based on the Euclidean distance performs poorly.

--

- If location difference is dominated by scale difference, .red[NN classifier assigns all observations to the population with smaller dispersion!]

---

# Mean Absolute Difference of Distances

* MADD (Sarkar and Ghosh 2019) is a semi-metric based on available data cloud, defined as: 

`$$\rho(\mathbf{x},\mathbf{y})= \frac{1}{n - 2} \sum_{\mathbf{z} \in \mathcal{X} \setminus \{\mathbf{x},\mathbf{y}\}} \big|\|\mathbf{x}- \mathbf{z}\| - \|\mathbf{y}-\mathbf{z}\|\big|$$`
* Roy et. al. 2022 used MADD for high dimension, low sample size classification problems.


* Computation of MADD between two points requires `\(\mathcal{O}(nd)\)` operations.


* Complexity becomes `\(\mathcal{O}(n^2d)\)` for classifying a single observation `\(\implies\)` Quadratic in `\(n\)`.

---

# High Dimensional Behavior of MADD

* If `\(\mathbf{X} \sim F_1, \mathbf{Y} \sim F_2\)` are two independent observations, then under certain conditions

`$$d^{-1/2}||\mathbf{X}-\mathbf{Y}|| \xrightarrow{P} \sqrt{\nu_{12}^2+\sigma_1^2+\sigma_2^2} \ \ \text{as} \ \ d \rightarrow \infty$$`

* Let us look at the expression,

`$$\rho_0(\mathbf{X},\mathbf{Y}) = d^{-1/2}\left[\frac{1}{n - 2} \sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathbf{X},\mathbf{Y}\}} \big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|\right]$$`

`$$= \frac{1}{n - 2} \left\{\sum_{\mathbf{Z} \in \mathcal{X}_1 \setminus \{\mathbf{X}\}} \underbrace{d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|}+
\sum_{\mathbf{Z} \in \mathcal{X}_2 \setminus \{\mathbf{Y}\}} d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|\right\}$$`

---

# High Dimensional Behavior of MADD

* If `\(\mathbf{X} \sim F_1, \mathbf{Y} \sim F_2\)` are two independent observations, then under certain conditions

`$$d^{-1/2}||\mathbf{X}-\mathbf{Y}|| \xrightarrow{P} \sqrt{\nu_{12}^2+\sigma_1^2+\sigma_2^2} \ \ \text{as} \ \ d \rightarrow \infty$$`
* Let us look at the expression,

`$$\rho_0(\mathbf{X},\mathbf{Y}) = d^{-1/2}\left[\frac{1}{n - 2} \sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathbf{X},\mathbf{Y}\}} \big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|\right]$$`

`$$= \frac{1}{n - 2} \left\{\sum_{\mathbf{Z} \in \mathcal{X}_1 \setminus \{\mathbf{X}\}} d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|+
\sum_{\mathbf{Z} \in \mathcal{X}_2 \setminus \{\mathbf{Y}\}} \underbrace{d^{-1/2}\big| \|\mathbf{X}- \mathbf{Z}\| - \|\mathbf{Y}- \mathbf{Z}\|\big|}\right\}$$`


---

# Modified Version of MADD

The modified version of MADD, will be of the form:

`$$\rho_{Mod}(\mathbf{x},\mathbf{y}) = \frac{1}{|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|} \sum_{\mathbf{z} \in \mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}} \big| \|\mathbf{x}- \mathbf{z}\| - \|\mathbf{y}- \mathbf{z}\|\big|$$`
Where,

   * `\(|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|\)` denotes the cardinality of `\(\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}\)`.
  
   * `\(\mathcal{X}^{\ast} \subset \mathcal{X}\)`.
  
   * `\(\mathcal{X}^{\ast} \cap \mathcal{X}_j \neq \phi, \quad \text{for } j= 1,2.\)`

---

# Strategy 01: SRSWOR

&lt;img src="SRSWOR.jpg" width="100%" style="display: block; margin: auto;" /&gt;

* .green[Straightforward choice]


* .red[Does not consider diversity in the data structure] `\(\implies\)` .red[May lack in representing the whole sample.]

---

# Strategy 02: Determinantal Point Process

.panelset[
.panel[.panel-name[DPP]
* A point process `\(\mathcal{P}\)` on a discrete set `\(\mathcal{Y}\)` `\(= \{1, . . . , N\}\)` is a probability measure on `\(2^{\mathcal{Y}}\)`, the set of all subsets of `\(\mathcal{Y}\)`. 

* It is said to be a DPP, if for a random set **Y** drawn as per `\(\mathcal{P}\)`, `\(\mathcal{P}(A \subseteq \boldsymbol{Y}) = det(K_A)\)` for every subset `\(A \subseteq \mathcal{Y}\)`.

`$$\mathcal{P}(i \in \boldsymbol{Y}) = K_{ii}, \quad \mathcal{P}(i \in \boldsymbol{Y}, j \in \boldsymbol{Y}) = K_{ii}K_{jj}-K_{ij}^2, \quad \text{for all }i,j \in \mathcal{Y}$$`
]
.panel[.panel-name[L-Ensemble]

* Defines a DPP through a positive semi-definite matrix `\(L\)` indexed by elements of `\(\mathcal{Y}\)`.
`$$\mathcal{P}_L(\boldsymbol{Y}= A) = \text{det}(L_A)/{\text{det}(L + I)}$$`

* Directly represent the probabilities of observing each subset of `\(\mathcal{Y}\)`.

]
.panel[.panel-name[k-DPP]

* A k-DPP selects exactly k points according to a DPP.

* DPP conditioned on the cardinality of the selected subset.
]
]

---
# Strategy 02: Determinantal Point Process

* .green[Considers the diversity in the data points.] 

* .red[Depends on the choice of Kernel Matrix.]

---
# Illustration of SRSWOR vs. DPP

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="WOR_vs_DPP.jpeg" alt="Figure: Points drawn from a SRSWOR (left) vs the same number of points sampled using DPP (right)." width="70%" /&gt;
&lt;p class="caption"&gt;Figure: Points drawn from a SRSWOR (left) vs the same number of points sampled using DPP (right).&lt;/p&gt;
&lt;/div&gt;

---

# DPP-1 

* Let us take `\(L= XX'\)`, where `\(X_{n \times p}\)` is the data matrix.

* Denote the rows of `\(X\)` by `\(\{\mathbf{X_i}\}_{i=1}^n\)`, `\(\mathcal{P}_L(\mathbf{Y}) \propto det(L_\mathbf{Y}) = Vol^2(\{\mathbf{X_i}\}_{i \in \mathbf{Y}})\)`.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="dpp_parallel.jpeg" alt="Figure: Geometrical View of DPPs (Source: Kulesza and Taskar 2012)" width="50%" /&gt;
&lt;p class="caption"&gt;Figure: Geometrical View of DPPs (Source: Kulesza and Taskar 2012)&lt;/p&gt;
&lt;/div&gt;

---

# DPP-2

* In a `\(2\)`-DPP problem, where `\(L=((L_{i,j}))\)`, with `\(L_{i,j}= e^{-\frac{||\mathbf{x_i}-\mathbf{x_j}||^2}{d}}\)`. 

* For the set `\(A=\{\mathbf{x_i},\mathbf{x_j}\}\)`, `$$\mathcal{P}_L(A) \propto det(L_A) = 1 - e^{-\frac{2||\mathbf{x_i}-\mathbf{x_j}||^2}{d}}$$`

* As the distance between `\(\mathbf{x_i}\)` and `\(\mathbf{x_j}\)` increases, the probability of their selection also increases.


* Thus we consider *Radial-Basis Function Kernel* between the data points present in each population.


* For two data points `\(\mathbf{x_i}\)` and `\(\mathbf{x_j}\)`, it is defined as `$$L(\mathbf{x_i},\mathbf{x_j})=e^{-\frac{||\mathbf{x_i}-\mathbf{x_j}||^2}{d}}$$`

* Being a kernel it will result in a positive semi-definite symmetric matrix (Lanckriet et al. 2002).

---

# Simulation Studies 

.green[Experiment Specifications:]

&gt;- `\(5\)`-nearest neighbor classifier

&gt;- Test set size: `\(500\)` ( `\(250\)` observations from each class)

&gt;- Sample sizes: `\(n_1 = n_2 = n = 50, 100\)`

&gt;- Dimensionality: `\(d = 20, 50, 100\)`

&gt;- No. of points Selected from each population: `\(k = 2, 4, 8, 16\)`

&gt;- `\(100\)` replications considered

---

### Simulation 01: A Pure Location Problem

* Population 1 `\(\equiv N_d(\mathbf{0},I_{d})\)` &amp; Population 2 `\(\equiv N_d(0.5\times \mathbf{1_{d}},I_{d})\)`


















.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-12-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-13-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-14-1.png" alt="Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Location Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
]

---

### Simulation 02: A Pure Scale Problem 

* Population 1 `\(\equiv N_d(\mathbf{0},I_{d})\)` &amp; Population 2 `\(\equiv N_d(\mathbf{0},2 \times I_{d})\)`



















.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-20-1.png" alt="Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-21-1.png" alt="Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-22-1.png" alt="Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rates (in %) in a Pure Scale Problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
]

---

###  Simulation 03: Location Problem with autocorrelated features


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-23-1.png" alt="Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-24-1.png" alt="Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-25-1.png" alt="Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) in a location Problem with autocorrelated features. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

## Comparison of computing times
















.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-31-1.png" alt="Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-32-1.png" alt="Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-33-1.png" alt="Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Figure: Computing Times taken by different Methods (in seconds). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

class: center, middle
background-image: url("BACK.jpg")
background-size: cover

# Scalable version of g-MADD

---


# Generalized MADD: An Overview

- Usual MADD is only confined to the cases where populations either differ in their location or in their total variance.

- For two vectors `\(\mathbf{x}\)` and `\(\mathbf{y}\)`, define,

`\begin{equation}
\label{1.1.4}
\rho_{h,\psi}(\mathbf{x}, \mathbf{y}) = \frac{1}{n - 2} \sum_{\mathbf{z} \in \mathcal{X} \setminus \{\mathbf{x},\mathbf{y}\}} \big| \phi_{h,\psi}(\mathbf{x}, \mathbf{z}) - \phi_{h,\psi}(\mathbf{y}, \mathbf{z}) \big|
\end{equation}`
Where,
  * `\(h : \mathbb{R}^+ \rightarrow \mathbb{R}^+\)` and `\(\psi : \mathbb{R}^+ \rightarrow \mathbb{R}^+\)` continuous, monotonically increasing, `\(h(0) = \psi(0) = 0\)` such that `\(\phi_{h,\psi}(\mathbf{x}, \mathbf{y}) = h\left( \frac{1}{d} \sum_{q=1}^{d} \psi \left( |x^{(q)} - y^{(q)}| \right) \right).\)` 

---
# Computational challenges with g-MADD
- Again, the computational complexity becomes `\(O(n^2d)\)` for classifying single observation.

--

- We will propose a scalable version of g-MADD of the form: `$$\rho^{Mod}_{h,\psi}(\mathbf{x},\mathbf{y}) = \frac{1}{|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|} \sum_{\mathbf{z} \in \mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}} \big| \phi_{h,\psi}(\mathbf{x}, \mathbf{z}) - \phi_{h,\psi}(\mathbf{y}, \mathbf{z})\big|$$`
  where `\(\mathcal{X}^{\ast} \text{ is a subset of } \mathcal{X}\)` such that `\(\mathcal{X}^{\ast} \cap \mathcal{X}_j \neq \phi, \quad \text{for } j=1,2.\)` 

---
# Choice of L-ensemble Matrix

### Generalization of DPP-1

- Euclidean distance is directly related to the volume of the parallelepiped.
- No such direct connection for a general distance.

### Generalization of DPP-2

- Recall DPP-2: Used `\(L(\mathbf{x_i}, \mathbf{x_j}) = e^{-\frac{||\mathbf{x_i} - \mathbf{x_j}||^2}{d}}\)`.

- For g-MADD, propose: `\(L(\mathbf{x_i}, \mathbf{x_j}) = e^{-\frac{1}{d} \sum_{q=1}^{d} \psi( |x_i^{(q)} - x_j^{(q)}|)}\)`.

- Replace Euclidean distance with a general distance function `\(\phi_{h,\psi}\)`. 

---
## Simulation: Features are independent normal vs features are independent t


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-34-1.png" alt="Misclassification rates (in %) when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-35-1.png" alt="Misclassification rates (in %) when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-36-1.png" alt="Misclassification rates (in %) when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

## Comparison of computing times


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-37-1.png" alt="Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-38-1.png" alt="Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 100]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-39-1.png" alt="Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Comparison of computing times when the features are independent normal vs when the features are independent t (with same mean and dispersion). The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---
## Comparison of scalable version of MADD and g-MADD: Misclassification Rate and Computing Time

&lt;img src="pgli1.png" width="100%" height="100%" style="display: block; margin: auto;" /&gt;

&lt;table class="table table-striped table-hover table-condensed table-responsive" style="color: black; width: auto !important; "&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Misclassification Rate (in %) &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; SE &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Computing Time (in sec.) &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; SE &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Traditional MADD &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1521 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.9966 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5592 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Traditional g-MADD &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.26 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1305 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.9581 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7411 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

class: center, middle
background-image: url("BACK.jpg")
background-size: cover

# Extension to multi-class classification problem

---

# Set-up in multi-class classification

- Consider a random sample `\(\mathcal{X}_j = \{\boldsymbol{X_{j1}}, \ldots, \boldsymbol{X_{jn_j}}\}\)` of size `\(n_j\)` from distribution function `\(F_j\)` on `\(\mathbb{R}^d\)`.

- Assume observations are i.i.d. within each sample.

- `\(\mathcal{X} = \bigcup_{j=1}^J \mathcal{X}_j\)` with `\(n = \sum_{j=1}^J n_j\)`.

- Each population `\(F_j\)` is characterized by a `\(d\)`-dimensional location vector `\(\boldsymbol{\mu}_{jd}\)`.

- Each population `\(F_j\)` also has a `\(d \times d\)` dispersion matrix `\(\boldsymbol{\Sigma}_{jd}\)`.

---
# High Dimensional Behavior

Now, let us look at the expression,
`$$\rho_0(\mathbf{X}_{ji},\mathbf{X}_{j'i'}) = d^{-1/2}\left[\frac{1}{n - 2} \sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathbf{X}_{ji},\mathbf{X}_{j'i'}\}} \big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|\right]$$`


`$$= \frac{1}{n - 2} \left\{\sum_{\mathbf{Z} \in \mathcal{X}_j \setminus \{\mathbf{X}_{ji}\}} \underbrace{d^{-1/2}\big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|}+
\sum_{\mathbf{Z} \in \mathcal{X}_{j'} \setminus \{\mathbf{X}_{j'i'}\}} d^{-1/2}\big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big| \\+
\sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathcal{X}_{j}\bigcup\mathcal{X}_{j'}\}} d^{-1/2}\big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|\right\}$$`

---
# High Dimensional Behavior

Now, let us look at the expression,
`$$\rho_0(\mathbf{X}_{ji},\mathbf{X}_{j'i'}) = d^{-1/2}\left[\frac{1}{n - 2} \sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathbf{X}_{ji},\mathbf{X}_{j'i'}\}} \big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|\right]$$`

`$$= \frac{1}{n - 2} \left\{\sum_{\mathbf{Z} \in \mathcal{X}_j \setminus \{\mathbf{X}_{ji}\}} d^{-1/2}\big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|+\sum_{\mathbf{Z} \in \mathcal{X}_{j'} \setminus \{\mathbf{X}_{j'i'}\}} \underbrace{d^{-1/2}\big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|} \\+
\sum_{\mathbf{Z} \in \mathcal{X} \setminus \{\mathcal{X}_{j}\bigcup\mathcal{X}_{j'}\}} d^{-1/2}\big| \|\mathbf{X}_{ji}- \mathbf{Z}\| - \|\mathbf{X}_{j'i'}- \mathbf{Z}\|\big|\right\}$$`
---

# Scalable Version of MADD

`$$\rho_{Mod}(\mathbf{x},\mathbf{y}) = \frac{1}{|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|} \sum_{\mathbf{z} \in \mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}} \big| \|\mathbf{x}- \mathbf{z}\| - \|\mathbf{y}- \mathbf{z}\|\big|$$`
Where,

   * `\(|\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}|\)` denotes the cardinality of `\(\mathcal{X}^{\ast}\setminus \{\mathbf{x},\mathbf{y}\}\)`.
  
   * `\(\mathcal{X}^{\ast} \subset \mathcal{X}\)`.
  
   * `\(\mathcal{X}^{\ast} \cap \mathcal{X}_j \neq \phi, \quad \text{for } j= 1,2,...,J.\)`

---

# Misclassification rate for a three-class pure location problem

.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-41-1.png" alt="Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-42-1.png" alt="Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) in a Pure Location Problem for a 3-class classification problem. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---
# Comparison of computing times


.panelset[
.panel[.panel-name[.green[d= 20]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-43-1.png" alt="Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]
.panel[.panel-name[.green[d= 50]]
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/unnamed-chunk-44-1.png" alt="Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Computing times taken (in secs.) in a 3-class classification problem with only location difference. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;
]]

---

class: center, middle

.pull-left[

# Application on benchmark dataset: Fashion MNIST

&gt;- Contains `\(60,000\)` `\(28 \times 28\)` grayscale images of the `\(10\)` fashion article classes.

&gt;- Also contains a test set of `\(10,000\)` images.
]

.pull-right[

&lt;img src="fashion-mnist-sprite.PNG" width="100%" style="display: block; margin: auto;" /&gt;
]

---
## Defining distance metric, MADD and g-MADD for Image Data

- Suppose each grayscale image has `\(p \times p\)` pixels, totaling `\(D = p^2\)` pixels.


- The distance between two images `\(I_1\)` and `\(I_2\)` is calculated as:
  `$$d(I_1, I_2) = \sqrt{\sum_{d=1}^D (I_1^d - I_2^d)^2}$$`

- This formula treats pixel values as feature values for each image.

--

- Following the same concept, MADD and g-MADD metrics between images are the usual MADD and g-MADD after vectorizing the images.

--

- Scalable versions of MADD and g-MADD are also consistent with the usual ones after vectorization.

---
## Two-class classification problem
- Randomly chosen `\(200\)` observations from `Sandal` and `\(200\)` observations from class `Sneaker` as training observations. 
&lt;img src="sandal_vs_sneaker.PNG" width="100%" style="display: block; margin: auto;" /&gt;

---

## Results

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/Figure-13-1.png" alt="Misclassification rates (in %) (left) and computing time taken (in sec.) (right) for classification of Sandal and Sneaker using 200 training observations (from each class) from MNIST FASHION dataset."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (in %) (left) and computing time taken (in sec.) (right) for classification of Sandal and Sneaker using 200 training observations (from each class) from MNIST FASHION dataset.&lt;/p&gt;
&lt;/div&gt;

---
## Three-class classification problem
- Randomly chosen `\(200\)` observations to construct the training set. 
&lt;img src="sandal_sneaker_ankle_boot.PNG" width="100%" style="display: block; margin: auto;" /&gt;

---
## Results
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="color: black; width: auto !important; "&gt;
&lt;caption&gt;Table: Misclassification Rate (in %) and Computing Time (in sec.) for classification of Sandal, Sneaker, Ankle boot using 200 training observations (from each class) from MNIST FASHION dataset.&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Misclassification Rate (in %) &lt;/th&gt;
   &lt;th style="text-align:right;font-weight: bold;background-color: rgba(211, 211, 211, 255) !important;"&gt; Computing Time (in sec.) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; DPP 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1551.882 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; DPP 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1554.737 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; SRSWOR &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1543.262 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Traditional MADD &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.40 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18766.605 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Euclidean &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 14.53 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15.577 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

class: center, middle
background-image: url("BACK.jpg")
background-size: cover

# Concluding Discussions

---
# DPP or SRSWOR?



.pull-left[


&lt;img src="WOR_vs_DPP.jpeg" width="100%" style="display: block; margin: auto;" /&gt;


]
.pull-right[

- DPP considers the diversity in the sample.

- DPP showed slight improvement in some cases.

- In real data analysis, DPP performed much better with the same `\(k\)`.

- Computing time for DPP and SRSWOR is very close.

- One may opt for the scalable version using DPP.

]
---
## Choice of k

- A crucial part of our Method is deciding the number of observations to draw from each population to balance accuracy and computing time.

--

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="Endsem_presentation_files/figure-html/Figure-11-1.png" alt="Misclassification rates (left) and computing time taken (in sec.) (right) for a pure scale problem with scalable version of MADD computed based on DPP-2. The reported numbers are averages `\(\pm\)` SE based on 100 replications."  /&gt;
&lt;p class="caption"&gt;Misclassification rates (left) and computing time taken (in sec.) (right) for a pure scale problem with scalable version of MADD computed based on DPP-2. The reported numbers are averages `\(\pm\)` SE based on 100 replications.&lt;/p&gt;
&lt;/div&gt;

---
## Comparison with existing Method for dealing with computational issues 

Pal et al. (2016) inspired from Condensed Nearest Neighbor (Hart 1968) or Reduced Nearest Neighbor (Gates 1972) used the following algorithm.

&lt;img src="CNN_final.PNG" width="80%" style="display: block; margin: auto;" /&gt;


---

## Comparison with existing Method for dealing with computational issues 

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pgli2.PNG" alt="Figure: Misclassification rate (in %) (left) and computing time taken (in sec.) (right) for a pure location problem with scalable version of MADD computed based on DPP-2 and based on CNN with varrying threshold." width="90%" /&gt;
&lt;p class="caption"&gt;Figure: Misclassification rate (in %) (left) and computing time taken (in sec.) (right) for a pure location problem with scalable version of MADD computed based on DPP-2 and based on CNN with varrying threshold.&lt;/p&gt;
&lt;/div&gt;

---

# References

* Gates, G. (1972). The reduced nearest neighbor rule (corresp.). *IEEE Transactions on Information Theory*, 18(3):431–433.

* Hall, P., Marron, J. S., and Neeman, A. (2005). Geometric representation of high dimension, low sample size data. *Journal of the Royal Statistical Society Series B*, 67(3):427–444.

* Hart, P. (1968). The condensed nearest neighbor rule. *IEEE Transactions on Information Theory*, 14(3):515–516.

* Pal, A. K., Mondal, P. K., and Ghosh, A. K. (2016). High dimensional nearest neighbor classification based on mean absolute differences of inter-point distances. *Pattern Recognition Letters*, 74:1–8.

* Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui, L., and Jordan, M. (2002). Learning the Kernel Matrix with Semi-Definite Programming. *Journal of Machine Learning Research*, 5:323–330.

---

# References

* Hall, P., Marron, J. S., and Neeman, A. (2005). Geometric representation of high dimension, low sample size data. *Journal of the Royal Statistical Society Series B*, 67(3):427–444.

* Hastie, T., Tibshirani, R., and Friedman, J. H. (2009). *The Elements of Statistical Learning. Springer Series in Statistics.* Springer, New York.

* Kulesza, A. and Taskar, B. (2012). Determinantal Point Processes for Machine Learning. *Foundations and Trends® in Machine Learning,* 5(2-3):123–286

* Sarkar, S. and Ghosh, A. (2019). On Perfect Clustering of High Dimension, Low Sample Size Data. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 42(9):2257–2272

* Roy, S., Sarkar, S., Dutta, S., and Ghosh, A. K. (2022). On Generalizations of Some Distance Based Classifiers for HDLSS Data. *Journal of Machine Learning Research*, 23(14):1–41.

---

class: center, middle
background-image: url("BACK.jpg")
background-size: cover

# Thank You

All codes are available in [Adrija211200/ScalableMADDR](https://github.com/Adrija211200/ScalableMADDR)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
